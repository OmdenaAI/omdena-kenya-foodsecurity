{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMModel_SPEI+Temperature.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HULfbkbn2XDt"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import sklearn\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from numpy import array\r\n",
        "from numpy import hstack\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Dense\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7EfqeCZ2r_t",
        "outputId": "1efb091d-bd09-4ad1-e3b8-83429862651e"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoEkmsnZ9XUB"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/data_senegal/senegal_weather_spei_1970_2020_025x025_v3.csv\")\r\n",
        "df = df.dropna()\r\n",
        "df = df.reset_index(drop=True)\r\n",
        "\r\n",
        "scaler = StandardScaler()\r\n",
        "data_scaled = scaler.fit_transform(df)\r\n",
        "data_scaled_withcol = pd.DataFrame(data_scaled,columns = df.columns) \r\n",
        "data_scaled_withcol['year'] = df['year']\r\n",
        "data_scaled_withcol['month'] = df['month']\r\n",
        "data_scaled_withcol['gpslat'] = df['gpslat']\r\n",
        "data_scaled_withcol['gpslon'] = df['gpslon']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlGfjDCj5HpM"
      },
      "source": [
        "in_seq_temp = array(df['temperature'])\r\n",
        "#in_seq_humidex = array(data_scaled_withcol[\"humidex\"])\r\n",
        "in_seq_prcp = array(df[\"total_precipitation\"])\r\n",
        "out_seq_spei = array(df[\"spei_1mon\"])\r\n",
        "\r\n",
        "in_seq_temp = in_seq_temp.reshape((len(in_seq_temp), 1))\r\n",
        "in_seq_prcp = in_seq_prcp.reshape((len(in_seq_prcp), 1))\r\n",
        "#in_seq_humidex = in_seq_humidex.reshape((len(in_seq_humidex), 1))\r\n",
        "out_seq_spei = out_seq_spei.reshape((len(out_seq_spei), 1))\r\n",
        "\r\n",
        "\r\n",
        "dataset = hstack((in_seq_temp, in_seq_prcp, out_seq_spei))\r\n",
        "\r\n",
        "n_steps = 12"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik8oMB9i-PbE"
      },
      "source": [
        "# split a multivariate sequence into samples\r\n",
        "def split_sequences(sequences, n_steps):\r\n",
        "  X, y = list(), list()\r\n",
        "  for i in range(len(sequences)):\r\n",
        "    # find the end of this pattern\r\n",
        "    end_ix = i + n_steps\r\n",
        "    # check if we are beyond the dataset\r\n",
        "    if end_ix > len(sequences):\r\n",
        "      break\r\n",
        "    # gather input and output parts of the pattern\r\n",
        "    seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\r\n",
        "    X.append(seq_x)\r\n",
        "    y.append(seq_y)\r\n",
        "  return array(X), array(y)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0Be8hGg_o0F"
      },
      "source": [
        "X, y = split_sequences(dataset, n_steps)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vgzcbbb_pn3"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTYrj1Zp5xGR",
        "outputId": "2ab53643-9b00-4d9e-e919-a00d487eb9b5"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147750, 12, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wO_6Kch6k7H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIAiTVb26k-f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb1asMUDe2DW",
        "outputId": "f979e7c8-f7f0-4f8a-899b-1359972e411a"
      },
      "source": [
        "n_features = X_train.shape[2]\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\r\n",
        "model.add(LSTM(200, activation='relu'))\r\n",
        "model.add(Dense(1))\r\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOWFmiU6fChe",
        "outputId": "33e3ddf6-0904-41e0-d9e9-2b45ff267cd6"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\r\n",
        "  model.fit(X_train, y_train, epochs=95, batch_size=64)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/95\n",
            "2309/2309 [==============================] - 91s 39ms/step - loss: 0.9686\n",
            "Epoch 2/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.4153\n",
            "Epoch 3/95\n",
            "2309/2309 [==============================] - 89s 39ms/step - loss: 0.3243\n",
            "Epoch 4/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.2720\n",
            "Epoch 5/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.2243\n",
            "Epoch 6/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.1884\n",
            "Epoch 7/95\n",
            "2309/2309 [==============================] - 89s 39ms/step - loss: 0.1622\n",
            "Epoch 8/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.1383\n",
            "Epoch 9/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.1208\n",
            "Epoch 10/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.1043\n",
            "Epoch 11/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.0912\n",
            "Epoch 12/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.0807\n",
            "Epoch 13/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.0724\n",
            "Epoch 14/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.0685\n",
            "Epoch 15/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.0632\n",
            "Epoch 16/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.0566\n",
            "Epoch 17/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.0530\n",
            "Epoch 18/95\n",
            "2309/2309 [==============================] - 89s 39ms/step - loss: 0.0503\n",
            "Epoch 19/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.0478\n",
            "Epoch 20/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.0439\n",
            "Epoch 21/95\n",
            "2309/2309 [==============================] - 91s 39ms/step - loss: 0.0433\n",
            "Epoch 22/95\n",
            "2309/2309 [==============================] - 91s 39ms/step - loss: 0.0401\n",
            "Epoch 23/95\n",
            "2309/2309 [==============================] - 91s 39ms/step - loss: 0.0385\n",
            "Epoch 24/95\n",
            "2309/2309 [==============================] - 92s 40ms/step - loss: 0.0374\n",
            "Epoch 25/95\n",
            "2309/2309 [==============================] - 91s 39ms/step - loss: 0.0351\n",
            "Epoch 26/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.0350\n",
            "Epoch 27/95\n",
            "2309/2309 [==============================] - 89s 38ms/step - loss: 0.0333\n",
            "Epoch 28/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0316\n",
            "Epoch 29/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0309\n",
            "Epoch 30/95\n",
            "2309/2309 [==============================] - 89s 38ms/step - loss: 0.0293\n",
            "Epoch 31/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0293\n",
            "Epoch 32/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0283\n",
            "Epoch 33/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0278\n",
            "Epoch 34/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0278\n",
            "Epoch 35/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0256\n",
            "Epoch 36/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0261\n",
            "Epoch 37/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0248\n",
            "Epoch 38/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0241\n",
            "Epoch 39/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0233\n",
            "Epoch 40/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0229\n",
            "Epoch 41/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0229\n",
            "Epoch 42/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0226\n",
            "Epoch 43/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0213\n",
            "Epoch 44/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0216\n",
            "Epoch 45/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0213\n",
            "Epoch 46/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0204\n",
            "Epoch 47/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0210\n",
            "Epoch 48/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0204\n",
            "Epoch 49/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0194\n",
            "Epoch 50/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0197\n",
            "Epoch 51/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0198\n",
            "Epoch 52/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0190\n",
            "Epoch 53/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0190\n",
            "Epoch 54/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0186\n",
            "Epoch 55/95\n",
            "2309/2309 [==============================] - 89s 38ms/step - loss: 0.0180\n",
            "Epoch 56/95\n",
            "2309/2309 [==============================] - 89s 39ms/step - loss: 0.0184\n",
            "Epoch 57/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0176\n",
            "Epoch 58/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0174\n",
            "Epoch 59/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0174\n",
            "Epoch 60/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0169\n",
            "Epoch 61/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0170\n",
            "Epoch 62/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0166\n",
            "Epoch 63/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0169\n",
            "Epoch 64/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0168\n",
            "Epoch 65/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0161\n",
            "Epoch 66/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0159\n",
            "Epoch 67/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0154\n",
            "Epoch 68/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0155\n",
            "Epoch 69/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0155\n",
            "Epoch 70/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0156\n",
            "Epoch 71/95\n",
            "2309/2309 [==============================] - 87s 38ms/step - loss: 0.0151\n",
            "Epoch 72/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0150\n",
            "Epoch 73/95\n",
            "2309/2309 [==============================] - 90s 39ms/step - loss: 0.0150\n",
            "Epoch 74/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0148\n",
            "Epoch 75/95\n",
            "2309/2309 [==============================] - 85s 37ms/step - loss: 0.0146\n",
            "Epoch 76/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0149\n",
            "Epoch 77/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0143\n",
            "Epoch 78/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0140\n",
            "Epoch 79/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0145\n",
            "Epoch 80/95\n",
            "2309/2309 [==============================] - 89s 39ms/step - loss: 0.0140\n",
            "Epoch 81/95\n",
            "2309/2309 [==============================] - 89s 38ms/step - loss: 0.0142\n",
            "Epoch 82/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0139\n",
            "Epoch 83/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0135\n",
            "Epoch 84/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0135\n",
            "Epoch 85/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0136\n",
            "Epoch 86/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0132\n",
            "Epoch 87/95\n",
            "2309/2309 [==============================] - 89s 38ms/step - loss: 0.0131\n",
            "Epoch 88/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0133\n",
            "Epoch 89/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0132\n",
            "Epoch 90/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0130\n",
            "Epoch 91/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0125\n",
            "Epoch 92/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0127\n",
            "Epoch 93/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0124\n",
            "Epoch 94/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0123\n",
            "Epoch 95/95\n",
            "2309/2309 [==============================] - 88s 38ms/step - loss: 0.0123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVCkZXulfWEJ",
        "outputId": "76e3a931-6371-4acc-93e2-0595ec84ab78"
      },
      "source": [
        "y_hat = model.predict(X_test)\r\n",
        "print(mean_squared_error(y_hat, y_test))\r\n",
        "print(sklearn.metrics.r2_score(y_hat, y_test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.018384891270588635\n",
            "0.9807274436450245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K549LalHMiDo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}